{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkXSnFt/C3NF1NaEvEaqah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArkS0001/Machine-Unlearning/blob/main/Machine_Unlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUwC5FgahTV",
        "outputId": "8d6383cf-f30d-4cee-805f-25a2dd886dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model score: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Hypothetical dataset\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "# Assume we need to forget the data point [3, 4]\n",
        "data_to_remove = [3, 4]\n",
        "index_to_remove = np.where((X == data_to_remove).all(axis=1))[0][0]\n",
        "\n",
        "# Remove the data point\n",
        "X_new = np.delete(X, index_to_remove, axis=0)\n",
        "y_new = np.delete(y, index_to_remove, axis=0)\n",
        "\n",
        "# Split the updated dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
        "\n",
        "# Retrain the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Verify the model\n",
        "print(\"Model score:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Verify the model's performance\n",
        "initial_score = model.score(X_test, y_test)\n",
        "print(\"Initial model score:\", initial_score)\n",
        "\n",
        "# Assume we need to forget the first data point in X_train\n",
        "data_to_remove = X_train[0]\n",
        "label_to_remove = y_train[0]\n",
        "\n",
        "# Identify the index of the data point to remove\n",
        "index_to_remove = np.where((X_train == data_to_remove).all(axis=1) & (y_train == label_to_remove))[0][0]\n",
        "\n",
        "# Remove the data point\n",
        "X_train_new = np.delete(X_train, index_to_remove, axis=0)\n",
        "y_train_new = np.delete(y_train, index_to_remove, axis=0)\n",
        "\n",
        "# Retrain the model on the updated dataset\n",
        "model.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Verify the model's performance after forgetting the data point\n",
        "final_score = model.score(X_test, y_test)\n",
        "print(\"Final model score:\", final_score)\n",
        "\n",
        "# Check if the model's performance is significantly affected\n",
        "if final_score < initial_score:\n",
        "    print(\"The model's performance decreased after unlearning the data point.\")\n",
        "else:\n",
        "    print(\"The model's performance remains stable after unlearning the data point.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC5aaxzVbT4T",
        "outputId": "dd63bd13-f252-4b26-90c1-0b126385f710"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model score: 1.0\n",
            "Final model score: 1.0\n",
            "The model's performance remains stable after unlearning the data point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression(max_iter=500)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Verify the model's performance\n",
        "initial_score = model.score(X_test, y_test)\n",
        "print(\"Initial model score:\", initial_score)\n",
        "\n",
        "# Assume we need to forget the first data point in X_train\n",
        "data_to_remove = X_train[0]\n",
        "label_to_remove = y_train[0]\n",
        "\n",
        "# Identify the index of the data point to remove\n",
        "index_to_remove = np.where((X_train == data_to_remove).all(axis=1) & (y_train == label_to_remove))[0][0]\n",
        "\n",
        "# Remove the data point\n",
        "X_train_new = np.delete(X_train, index_to_remove, axis=0)\n",
        "y_train_new = np.delete(y_train, index_to_remove, axis=0)\n",
        "\n",
        "# Retrain the model on the updated dataset\n",
        "model.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Verify the model's performance after forgetting the data point\n",
        "final_score = model.score(X_test, y_test)\n",
        "print(\"Final model score:\", final_score)\n",
        "\n",
        "# Check if the model's performance is significantly affected\n",
        "if final_score < initial_score:\n",
        "    print(\"The model's performance decreased after unlearning the data point.\")\n",
        "else:\n",
        "    print(\"The model's performance remains stable after unlearning the data point.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCXr6OKnbinV",
        "outputId": "ac6ebe8a-3773-48ad-a054-4e6b42c50136"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model score: 0.975\n",
            "Final model score: 0.975\n",
            "The model's performance remains stable after unlearning the data point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define a simple neural network model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = create_model()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, verbose=2)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Initial model accuracy: {initial_accuracy:.4f}\")\n",
        "\n",
        "# Assume we need to forget the first data point in X_train\n",
        "index_to_remove = 0\n",
        "\n",
        "# Remove the data point\n",
        "X_train_new = np.delete(X_train, index_to_remove, axis=0)\n",
        "y_train_new = np.delete(y_train, index_to_remove, axis=0)\n",
        "\n",
        "# Retrain the model on the updated dataset\n",
        "model = create_model()\n",
        "model.fit(X_train_new, y_train_new, epochs=5, batch_size=128, validation_split=0.2, verbose=2)\n",
        "\n",
        "# Evaluate the model after forgetting the data point\n",
        "final_loss, final_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Final model accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Check if the model's performance is significantly affected\n",
        "if final_accuracy < initial_accuracy:\n",
        "    print(\"The model's performance decreased after unlearning the data point.\")\n",
        "else:\n",
        "    print(\"The model's performance remains stable after unlearning the data point.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w16A_CHfbrLk",
        "outputId": "0bd705b6-8a40-4211-ff12-7a6867090208"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.8937 - loss: 0.3913 - val_accuracy: 0.9453 - val_loss: 0.2065\n",
            "Epoch 2/5\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9478 - loss: 0.1831 - val_accuracy: 0.9568 - val_loss: 0.1590\n",
            "Epoch 3/5\n",
            "375/375 - 2s - 4ms/step - accuracy: 0.9619 - loss: 0.1329 - val_accuracy: 0.9624 - val_loss: 0.1298\n",
            "Epoch 4/5\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9708 - loss: 0.1030 - val_accuracy: 0.9679 - val_loss: 0.1117\n",
            "Epoch 5/5\n",
            "375/375 - 4s - 10ms/step - accuracy: 0.9763 - loss: 0.0842 - val_accuracy: 0.9667 - val_loss: 0.1094\n",
            "Initial model accuracy: 0.9689\n",
            "Epoch 1/5\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.8926 - loss: 0.3917 - val_accuracy: 0.9388 - val_loss: 0.2175\n",
            "Epoch 2/5\n",
            "375/375 - 2s - 5ms/step - accuracy: 0.9475 - loss: 0.1832 - val_accuracy: 0.9538 - val_loss: 0.1611\n",
            "Epoch 3/5\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.9619 - loss: 0.1316 - val_accuracy: 0.9606 - val_loss: 0.1312\n",
            "Epoch 4/5\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.9710 - loss: 0.1036 - val_accuracy: 0.9663 - val_loss: 0.1145\n",
            "Epoch 5/5\n",
            "375/375 - 2s - 5ms/step - accuracy: 0.9758 - loss: 0.0847 - val_accuracy: 0.9694 - val_loss: 0.1037\n",
            "Final model accuracy: 0.9715\n",
            "The model's performance remains stable after unlearning the data point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Step 1: Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the data\n",
        "\n",
        "# Step 2: Define and train the initial model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "initial_model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "initial_model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = initial_model.evaluate(x_test, y_test)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 3: Identify and remove data related to digit '5'\n",
        "print(\"Identifying data to unlearn...\")\n",
        "indices_to_unlearn = np.where(y_train == 5)[0]  # Indices where the label is 5\n",
        "\n",
        "x_train_unlearned = np.delete(x_train, indices_to_unlearn, axis=0)  # Remove these indices\n",
        "y_train_unlearned = np.delete(y_train, indices_to_unlearn, axis=0)\n",
        "\n",
        "# Step 4: Retrain the model on the modified dataset\n",
        "unlearned_model = create_model()\n",
        "print(\"Retraining Model Without Digit '5'...\")\n",
        "unlearned_model.fit(x_train_unlearned, y_train_unlearned, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the unlearned model\n",
        "unlearned_loss, unlearned_accuracy = unlearned_model.evaluate(x_test, y_test)\n",
        "print(f\"Unlearned Model Accuracy (General): {unlearned_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Validate forgetting on the removed class (digit '5')\n",
        "print(\"Evaluating forgetting on Digit '5'...\")\n",
        "test_indices = np.where(y_test == 5)[0]\n",
        "x_test_digit_5 = x_test[test_indices]\n",
        "y_test_digit_5 = y_test[test_indices]\n",
        "\n",
        "digit_5_loss, digit_5_accuracy = unlearned_model.evaluate(x_test_digit_5, y_test_digit_5)\n",
        "print(f\"Accuracy on Digit '5': {digit_5_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Validate on other digits (digits 0-9 excluding 5)\n",
        "print(\"Evaluating model on all other digits...\")\n",
        "other_test_indices = np.where(y_test != 5)[0]\n",
        "x_test_other_digits = x_test[other_test_indices]\n",
        "y_test_other_digits = y_test[other_test_indices]\n",
        "\n",
        "other_loss, other_accuracy = unlearned_model.evaluate(x_test_other_digits, y_test_other_digits)\n",
        "print(f\"Accuracy on Other Digits: {other_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx4kaeHacsT4",
        "outputId": "16678767-4c5e-47e5-efcf-a50af6e31f5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8710 - loss: 0.4552 - val_accuracy: 0.9585 - val_loss: 0.1479\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9611 - loss: 0.1330 - val_accuracy: 0.9743 - val_loss: 0.0923\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9743 - loss: 0.0861 - val_accuracy: 0.9758 - val_loss: 0.0877\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9802 - loss: 0.0644 - val_accuracy: 0.9782 - val_loss: 0.0806\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0486 - val_accuracy: 0.9793 - val_loss: 0.0777\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0869\n",
            "Initial Model Accuracy: 97.56%\n",
            "Identifying data to unlearn...\n",
            "Retraining Model Without Digit '5'...\n",
            "Epoch 1/5\n",
            "\u001b[1m1536/1536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8825 - loss: 0.4188 - val_accuracy: 0.9626 - val_loss: 0.1228\n",
            "Epoch 2/5\n",
            "\u001b[1m1536/1536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.1161 - val_accuracy: 0.9758 - val_loss: 0.0846\n",
            "Epoch 3/5\n",
            "\u001b[1m1536/1536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9769 - loss: 0.0766 - val_accuracy: 0.9753 - val_loss: 0.0743\n",
            "Epoch 4/5\n",
            "\u001b[1m1536/1536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9836 - loss: 0.0545 - val_accuracy: 0.9824 - val_loss: 0.0601\n",
            "Epoch 5/5\n",
            "\u001b[1m1536/1536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0419 - val_accuracy: 0.9815 - val_loss: 0.0592\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8850 - loss: 1.6440\n",
            "Unlearned Model Accuracy (General): 88.99%\n",
            "Evaluating forgetting on Digit '5'...\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 17.1471\n",
            "Accuracy on Digit '5': 0.00%\n",
            "Evaluating model on all other digits...\n",
            "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0906\n",
            "Accuracy on Other Digits: 97.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate machine unlearning with a synthetic dataset similar to the training of a Language Model (LLM), we can create a dataset that consists of sentences paired with labels or features. We'll use a neural network to classify these sentences based on certain keywords and then unlearn specific keywords.\n",
        "\n",
        "Here’s the plan:\n",
        "\n",
        "    Create Synthetic Data:\n",
        "        Sentences with labels based on certain keywords.\n",
        "        Example: Sentences like \"The dog barked loudly\" labeled animal, and \"The car drove fast\" labeled vehicle.\n",
        "\n",
        "    Train a Neural Network:\n",
        "        Train a simple text classifier on this synthetic dataset.\n",
        "\n",
        "    Unlearn Specific Information:\n",
        "        Remove all examples related to a specific class (e.g., vehicle) and retrain the model to \"forget\" it.\n",
        "\n",
        "    Validate Unlearning:\n",
        "        Evaluate the model on sentences containing the removed class and on sentences from remaining classes.\n",
        "  \n",
        "  How It Works:\n",
        "\n",
        "    Synthetic Dataset:\n",
        "        Sentences belong to animal, vehicle, or nature classes.\n",
        "        Tokenizer processes the sentences for the neural network.\n",
        "    Initial Training:\n",
        "        The model learns to classify sentences into their respective classes.\n",
        "    Unlearning:\n",
        "        All sentences labeled vehicle are removed from the training set.\n",
        "        The model is retrained on the reduced dataset to \"forget\" about vehicles.\n",
        "    Validation:\n",
        "        Performance on the vehicle class drops significantly.\n",
        "        Accuracy on the remaining classes stays relatively high.\n",
        "\n",
        "Expected Results:\n",
        "\n",
        "    Initial Model:\n",
        "        High accuracy on all classes.\n",
        "    Unlearned Model:\n",
        "        Poor accuracy on the vehicle class (indicating forgetting).\n",
        "        Retains accuracy on the other classes (animal and nature)."
      ],
      "metadata": {
        "id": "7QP8J0Mhdn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "initial_model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "initial_model.fit(x_train, y_train, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = initial_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Unlearning process (remove \"vehicle\" class data)\n",
        "print(\"Unlearning 'vehicle' class...\")\n",
        "indices_to_unlearn = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "\n",
        "x_train_unlearned = np.delete(x_train, indices_to_unlearn, axis=0)\n",
        "y_train_unlearned = np.delete(y_train, indices_to_unlearn, axis=0)\n",
        "\n",
        "# Retrain the model\n",
        "unlearned_model = create_model()\n",
        "print(\"Retraining Model Without 'vehicle' class...\")\n",
        "unlearned_model.fit(x_train_unlearned, y_train_unlearned, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the unlearned model on general test data\n",
        "unlearned_loss, unlearned_accuracy = unlearned_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Unlearned Model Accuracy (General): {unlearned_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Validate forgetting of \"vehicle\" class\n",
        "print(\"Evaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_sentences = [sentence for sentence, label in data if label == \"vehicle\"]\n",
        "vehicle_sequences = tokenizer.texts_to_sequences(vehicle_sentences)\n",
        "vehicle_padded = pad_sequences(vehicle_sequences, maxlen=max_length, padding=\"post\")\n",
        "vehicle_labels = np.array([label_map[\"vehicle\"]] * len(vehicle_sentences))\n",
        "\n",
        "vehicle_loss, vehicle_accuracy = unlearned_model.evaluate(vehicle_padded, vehicle_labels, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Validate on remaining classes\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = unlearned_model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVHjo31UdowD",
        "outputId": "609c108b-7f34-44e8-8ea0-7aa3dcbb5e77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6310 - loss: 1.0975\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7262 - loss: 1.0938\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6429 - loss: 1.0937\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3690 - loss: 1.0902\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4524 - loss: 1.0871\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3690 - loss: 1.0881\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4524 - loss: 1.0827\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3690 - loss: 1.0807 \n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4524 - loss: 1.0740\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3690 - loss: 1.0775\n",
            "Initial Model Accuracy: 0.00%\n",
            "Unlearning 'vehicle' class...\n",
            "Retraining Model Without 'vehicle' class...\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 1.0992\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 1.0947\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 1.0903\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 1.0859\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 1.0815\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 1.0770\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 1.0724\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 1.0677\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 1.0629\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 1.0579\n",
            "Unlearned Model Accuracy (General): 100.00%\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "Accuracy on Remaining Classes: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forgetting Without Retraining**"
      ],
      "metadata": {
        "id": "Jh2o7yo-eEmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define and train the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Perform machine unlearning on the 'vehicle' class\n",
        "print(\"Forgetting 'vehicle' class...\")\n",
        "\n",
        "vehicle_indices = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "x_vehicle = x_train[vehicle_indices]\n",
        "y_vehicle = y_train[vehicle_indices]\n",
        "\n",
        "# Use gradient ascent to reduce accuracy on the 'vehicle' class\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(10):  # Perform 10 gradient ascent steps\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_vehicle, training=True)\n",
        "        loss = -loss_function(y_vehicle, predictions)  # Negative loss for gradient ascent\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch + 1}: Negative Loss = {-loss.numpy():.4f}\")\n",
        "\n",
        "# Step 5: Validate forgetting\n",
        "print(\"Evaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_sequences = tokenizer.texts_to_sequences(\n",
        "    [sentence for sentence, label in data if label == \"vehicle\"]\n",
        ")\n",
        "vehicle_padded = pad_sequences(vehicle_sequences, maxlen=max_length, padding=\"post\")\n",
        "vehicle_labels = np.array([label_map[\"vehicle\"]] * len(vehicle_sequences))\n",
        "\n",
        "vehicle_loss, vehicle_accuracy = model.evaluate(vehicle_padded, vehicle_labels, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Validate on other classes\n",
        "print(\"Evaluating on remaining classes...\")\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BghaHvZodulG",
        "outputId": "edd04c31-8ab8-49c2-d00b-290b9f9b5dc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.1786 - loss: 1.0998\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2738 - loss: 1.0974 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4643 - loss: 1.0955 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7262 - loss: 1.0928\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5476 - loss: 1.0908\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6310 - loss: 1.0866 \n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4524 - loss: 1.0857 \n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3690 - loss: 1.0834\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4524 - loss: 1.0783 \n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3690 - loss: 1.0788 \n",
            "Initial Model Accuracy: 0.00%\n",
            "Forgetting 'vehicle' class...\n",
            "Epoch 1: Negative Loss = 1.0371\n",
            "Epoch 2: Negative Loss = 1.1220\n",
            "Epoch 3: Negative Loss = 1.1940\n",
            "Epoch 4: Negative Loss = 1.2900\n",
            "Epoch 5: Negative Loss = 1.4422\n",
            "Epoch 6: Negative Loss = 1.6972\n",
            "Epoch 7: Negative Loss = 2.1405\n",
            "Epoch 8: Negative Loss = 2.9237\n",
            "Epoch 9: Negative Loss = 4.2501\n",
            "Epoch 10: Negative Loss = 6.1930\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "Evaluating on remaining classes...\n",
            "Accuracy on Remaining Classes: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define and train the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Perform machine unlearning on the 'vehicle' class\n",
        "print(\"Forgetting 'vehicle' class...\")\n",
        "\n",
        "vehicle_indices = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "x_vehicle = x_train[vehicle_indices]\n",
        "y_vehicle = y_train[vehicle_indices]\n",
        "\n",
        "# Use gradient ascent to reduce accuracy on the 'vehicle' class\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(10):  # Perform 10 gradient ascent steps\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_vehicle, training=True)\n",
        "        loss = -loss_function(y_vehicle, predictions)  # Negative loss for gradient ascent\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch + 1}: Negative Loss = {-loss.numpy():.4f}\")\n",
        "\n",
        "# Step 5: Sample Predictions Before and After Forgetting\n",
        "def make_prediction(model, sentence):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label = list(label_map.keys())[np.argmax(prediction)]\n",
        "    return predicted_label\n",
        "\n",
        "sample_sentences = [\n",
        "    \"The dog barked loudly\",  # animal\n",
        "    \"The car drove fast\",     # vehicle\n",
        "    \"The tree swayed in the wind\"  # nature\n",
        "]\n",
        "\n",
        "print(\"\\nSample Predictions Before Forgetting:\")\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n",
        "\n",
        "# Validate forgetting on 'vehicle' class\n",
        "print(\"\\nEvaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_sequences = tokenizer.texts_to_sequences(\n",
        "    [sentence for sentence, label in data if label == \"vehicle\"]\n",
        ")\n",
        "vehicle_padded = pad_sequences(vehicle_sequences, maxlen=max_length, padding=\"post\")\n",
        "vehicle_labels = np.array([label_map[\"vehicle\"]] * len(vehicle_sequences))\n",
        "\n",
        "vehicle_loss, vehicle_accuracy = model.evaluate(vehicle_padded, vehicle_labels, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Validate on remaining classes\n",
        "print(\"Evaluating on remaining classes...\")\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nSample Predictions After Forgetting:\")\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDFbw6Bme0Sd",
        "outputId": "d5837bfd-76ba-4a63-d9bc-15f20f9e77ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.3571 - loss: 1.1008\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2738 - loss: 1.0981 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5476 - loss: 1.0951\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7262 - loss: 1.0917\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7262 - loss: 1.0889\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0849\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4524 - loss: 1.0824\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4524 - loss: 1.0775\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3690 - loss: 1.0790\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3690 - loss: 1.0747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 1489 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7b8103d9f010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Accuracy: 0.00%\n",
            "Forgetting 'vehicle' class...\n",
            "Epoch 1: Negative Loss = 1.0237\n",
            "Epoch 2: Negative Loss = 1.1196\n",
            "Epoch 3: Negative Loss = 1.1989\n",
            "Epoch 4: Negative Loss = 1.2937\n",
            "Epoch 5: Negative Loss = 1.4331\n",
            "Epoch 6: Negative Loss = 1.6559\n",
            "Epoch 7: Negative Loss = 2.0284\n",
            "Epoch 8: Negative Loss = 2.6695\n",
            "Epoch 9: Negative Loss = 3.7595\n",
            "Epoch 10: Negative Loss = 5.4641\n",
            "\n",
            "Sample Predictions Before Forgetting:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: animal\n",
            "\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "Evaluating on remaining classes...\n",
            "Accuracy on Remaining Classes: 50.00%\n",
            "\n",
            "Sample Predictions After Forgetting:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: animal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define and train the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Perform balanced machine unlearning on the 'vehicle' class\n",
        "print(\"\\nForgetting 'vehicle' class (Balanced)...\")\n",
        "\n",
        "# Get indices of 'vehicle' class\n",
        "vehicle_indices = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "x_vehicle = x_train[vehicle_indices]\n",
        "y_vehicle = y_train[vehicle_indices]\n",
        "\n",
        "# Gradient ascent for forgetting\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "for epoch in range(20):  # Perform 20 smaller gradient ascent steps\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_vehicle, training=True)\n",
        "        loss = -loss_function(y_vehicle, predictions)  # Negative loss for gradient ascent\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Log the performance periodically\n",
        "    if epoch % 5 == 0 or epoch == 19:\n",
        "        vehicle_loss, vehicle_accuracy = model.evaluate(\n",
        "            x_vehicle, y_vehicle, verbose=0\n",
        "        )\n",
        "        print(f\"Epoch {epoch + 1}: Accuracy on 'vehicle' class = {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Sample Predictions Before and After Forgetting\n",
        "def make_prediction(model, sentence):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label = list(label_map.keys())[np.argmax(prediction)]\n",
        "    return predicted_label\n",
        "\n",
        "sample_sentences = [\n",
        "    \"The dog barked loudly\",  # animal\n",
        "    \"The car drove fast\",     # vehicle\n",
        "    \"The tree swayed in the wind\"  # nature\n",
        "]\n",
        "\n",
        "print(\"\\nSample Predictions After Balanced Forgetting:\")\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n",
        "\n",
        "# Evaluate forgetting effectiveness\n",
        "print(\"\\nEvaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_loss, vehicle_accuracy = model.evaluate(x_vehicle, y_vehicle, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nEvaluating on remaining classes...\")\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP3UNsQxfeA4",
        "outputId": "a9c13a67-622a-4d5c-ea60-ea83027650fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5357 - loss: 1.0964\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3690 - loss: 1.0942\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0902\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0864\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0825\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0814\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5476 - loss: 1.0777\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5476 - loss: 1.0764\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5476 - loss: 1.0681\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6310 - loss: 1.0635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7b81119b1870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Accuracy: 0.00%\n",
            "\n",
            "Forgetting 'vehicle' class (Balanced)...\n",
            "Epoch 1: Accuracy on 'vehicle' class = 100.00%\n",
            "Epoch 6: Accuracy on 'vehicle' class = 66.67%\n",
            "Epoch 11: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 16: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 20: Accuracy on 'vehicle' class = 0.00%\n",
            "\n",
            "Sample Predictions After Balanced Forgetting:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: animal\n",
            "\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "\n",
            "Evaluating on remaining classes...\n",
            "Accuracy on Remaining Classes: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define and train the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Perform machine unlearning on the 'vehicle' class\n",
        "print(\"\\nForgetting 'vehicle' class with RMSprop and Gradient Clipping...\")\n",
        "\n",
        "# Get indices of 'vehicle' class\n",
        "vehicle_indices = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "x_vehicle = x_train[vehicle_indices]\n",
        "y_vehicle = y_train[vehicle_indices]\n",
        "\n",
        "# Define optimizer with gradient clipping\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, clipnorm=1.0)\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "for epoch in range(20):  # Perform 20 gradient ascent steps\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_vehicle, training=True)\n",
        "        loss = -loss_function(y_vehicle, predictions)  # Negative loss for gradient ascent\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Log the performance periodically\n",
        "    if epoch % 5 == 0 or epoch == 19:\n",
        "        vehicle_loss, vehicle_accuracy = model.evaluate(\n",
        "            x_vehicle, y_vehicle, verbose=0\n",
        "        )\n",
        "        print(f\"Epoch {epoch + 1}: Accuracy on 'vehicle' class = {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Sample Predictions After Forgetting\n",
        "def make_prediction(model, sentence):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label = list(label_map.keys())[np.argmax(prediction)]\n",
        "    return predicted_label\n",
        "\n",
        "sample_sentences = [\n",
        "    \"The dog barked loudly\",  # animal\n",
        "    \"The car drove fast\",     # vehicle\n",
        "    \"The tree swayed in the wind\"  # nature\n",
        "]\n",
        "\n",
        "print(\"\\nSample Predictions After Forgetting:\")\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n",
        "\n",
        "# Evaluate forgetting effectiveness\n",
        "print(\"\\nEvaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_loss, vehicle_accuracy = model.evaluate(x_vehicle, y_vehicle, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nEvaluating on remaining classes...\")\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSi-7kEggKss",
        "outputId": "9ecaeef4-e103-4ba2-8638-e5b4c0883e35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.0952 - loss: 1.1014  \n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3690 - loss: 1.0980 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6310 - loss: 1.0948 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3690 - loss: 1.0957 \n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3690 - loss: 1.0930\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4524 - loss: 1.0893\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5357 - loss: 1.0834\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4524 - loss: 1.0834\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4524 - loss: 1.0816\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3690 - loss: 1.0840\n",
            "Initial Model Accuracy: 0.00%\n",
            "\n",
            "Forgetting 'vehicle' class with RMSprop and Gradient Clipping...\n",
            "Epoch 1: Accuracy on 'vehicle' class = 100.00%\n",
            "Epoch 6: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 11: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 16: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 20: Accuracy on 'vehicle' class = 0.00%\n",
            "\n",
            "Sample Predictions After Forgetting:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: animal\n",
            "\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "\n",
            "Evaluating on remaining classes...\n",
            "Accuracy on Remaining Classes: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import random\n",
        "\n",
        "# Function to augment data\n",
        "def augment_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) > 1:\n",
        "        random.shuffle(words)  # Shuffle words for variation\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Step 1: Create and augment synthetic dataset\n",
        "original_data = [\n",
        "    (\"The dog barked loudly\", \"animal\"),\n",
        "    (\"The cat climbed the tree\", \"animal\"),\n",
        "    (\"The bird sang beautifully\", \"animal\"),\n",
        "    (\"The car drove fast\", \"vehicle\"),\n",
        "    (\"The truck carried heavy load\", \"vehicle\"),\n",
        "    (\"The bike sped past the car\", \"vehicle\"),\n",
        "    (\"The flower bloomed in spring\", \"nature\"),\n",
        "    (\"The tree swayed in the wind\", \"nature\"),\n",
        "    (\"The river flowed gently\", \"nature\"),\n",
        "]\n",
        "\n",
        "# Augment and expand dataset\n",
        "augmented_data = []\n",
        "for _ in range(100):  # Repeat dataset 100 times with random augmentations\n",
        "    for sentence, label in original_data:\n",
        "        augmented_data.append((augment_sentence(sentence), label))\n",
        "\n",
        "# Shuffle augmented dataset\n",
        "random.shuffle(augmented_data)\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences, labels = zip(*augmented_data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define and train the neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 16, input_length=max_length),\n",
        "        LSTM(32),\n",
        "        Dense(len(label_map), activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Sample Predictions Before Forgetting\n",
        "print(\"\\nSample Predictions Before Forgetting:\")\n",
        "sample_sentences = [\n",
        "    \"The dog barked loudly\",  # animal\n",
        "    \"The car drove fast\",     # vehicle\n",
        "    \"The tree swayed in the wind\"  # nature\n",
        "]\n",
        "\n",
        "def make_prediction(model, sentence):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label = list(label_map.keys())[np.argmax(prediction)]\n",
        "    return predicted_label\n",
        "\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n",
        "\n",
        "\n",
        "# Train the initial model\n",
        "model = create_model()\n",
        "print(\"Training Initial Model...\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the initial model\n",
        "initial_loss, initial_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Initial Model Accuracy: {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Perform machine unlearning on the 'vehicle' class\n",
        "print(\"\\nForgetting 'vehicle' class with RMSprop and Gradient Clipping...\")\n",
        "\n",
        "# Get indices of 'vehicle' class\n",
        "vehicle_indices = [i for i, label in enumerate(y_train) if label == label_map[\"vehicle\"]]\n",
        "x_vehicle = x_train[vehicle_indices]\n",
        "y_vehicle = y_train[vehicle_indices]\n",
        "\n",
        "# Define optimizer with gradient clipping\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, clipnorm=1.0)\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "for epoch in range(20):  # Perform 20 gradient ascent steps\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_vehicle, training=True)\n",
        "        loss = -loss_function(y_vehicle, predictions)  # Negative loss for gradient ascent\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Log the performance periodically\n",
        "    if epoch % 5 == 0 or epoch == 19:\n",
        "        vehicle_loss, vehicle_accuracy = model.evaluate(\n",
        "            x_vehicle, y_vehicle, verbose=0\n",
        "        )\n",
        "        print(f\"Epoch {epoch + 1}: Accuracy on 'vehicle' class = {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Sample Predictions After Forgetting\n",
        "def make_prediction(model, sentence):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded = pad_sequences(sequence, maxlen=max_length, padding=\"post\")\n",
        "    prediction = model.predict(padded)\n",
        "    predicted_label = list(label_map.keys())[np.argmax(prediction)]\n",
        "    return predicted_label\n",
        "\n",
        "sample_sentences = [\n",
        "    \"The dog barked loudly\",  # animal\n",
        "    \"The car drove fast\",     # vehicle\n",
        "    \"The tree swayed in the wind\"  # nature\n",
        "]\n",
        "\n",
        "print(\"\\nSample Predictions After Forgetting:\")\n",
        "for sentence in sample_sentences:\n",
        "    predicted_label = make_prediction(model, sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Label: {predicted_label}\")\n",
        "\n",
        "# Evaluate forgetting effectiveness\n",
        "print(\"\\nEvaluating forgetting on 'vehicle' class...\")\n",
        "vehicle_loss, vehicle_accuracy = model.evaluate(x_vehicle, y_vehicle, verbose=0)\n",
        "print(f\"Accuracy on 'vehicle' class: {vehicle_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nEvaluating on remaining classes...\")\n",
        "remaining_indices = [i for i, label in enumerate(y_test) if label != label_map[\"vehicle\"]]\n",
        "x_test_remaining = x_test[remaining_indices]\n",
        "y_test_remaining = y_test[remaining_indices]\n",
        "\n",
        "remaining_loss, remaining_accuracy = model.evaluate(x_test_remaining, y_test_remaining, verbose=0)\n",
        "print(f\"Accuracy on Remaining Classes: {remaining_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jIvQ6Q8g1ok",
        "outputId": "4ebeab51-f6ae-4af2-8edc-4eef31538d6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5283 - loss: 1.0921\n",
            "Epoch 2/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.9845\n",
            "Epoch 3/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.5515\n",
            "Epoch 4/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0912\n",
            "Epoch 5/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0187\n",
            "Epoch 6/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0100\n",
            "Epoch 7/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0071\n",
            "Epoch 8/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0052\n",
            "Epoch 9/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0043\n",
            "Epoch 10/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0035\n",
            "Initial Model Accuracy: 100.00%\n",
            "\n",
            "Sample Predictions Before Forgetting:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b81104d3f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: vehicle\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: nature\n",
            "Training Initial Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6330 - loss: 1.0857\n",
            "Epoch 2/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.9528\n",
            "Epoch 3/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.4907\n",
            "Epoch 4/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1014\n",
            "Epoch 5/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0202\n",
            "Epoch 6/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0090\n",
            "Epoch 7/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0062\n",
            "Epoch 8/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0043\n",
            "Epoch 9/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0036\n",
            "Epoch 10/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0028\n",
            "Initial Model Accuracy: 100.00%\n",
            "\n",
            "Forgetting 'vehicle' class with RMSprop and Gradient Clipping...\n",
            "Epoch 1: Accuracy on 'vehicle' class = 100.00%\n",
            "Epoch 6: Accuracy on 'vehicle' class = 100.00%\n",
            "Epoch 11: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 16: Accuracy on 'vehicle' class = 0.00%\n",
            "Epoch 20: Accuracy on 'vehicle' class = 0.00%\n",
            "\n",
            "Sample Predictions After Forgetting:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b81037e57e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "Sentence: 'The dog barked loudly' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Sentence: 'The car drove fast' -> Predicted Label: animal\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Sentence: 'The tree swayed in the wind' -> Predicted Label: animal\n",
            "\n",
            "Evaluating forgetting on 'vehicle' class...\n",
            "Accuracy on 'vehicle' class: 0.00%\n",
            "\n",
            "Evaluating on remaining classes...\n",
            "Accuracy on Remaining Classes: 96.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to remove specific knowledge or data from the model, making it \"forget\" certain aspects of the training process while preserving the rest of the model's performance on other tasks. This can be particularly useful in contexts like:\n",
        "\n",
        "    Data Privacy:\n",
        "        GDPR Compliance: When users request the deletion of their data, models may need to forget the specific information associated with those users without retraining from scratch. Machine unlearning can help with compliance by removing sensitive data from the model.\n",
        "        Right to be Forgotten: In situations where a user wishes to withdraw consent for data usage, machine unlearning allows models to forget specific user information.\n",
        "\n",
        "    Model Optimization:\n",
        "        Memory Efficiency: Instead of retraining a model entirely, unlearning allows for more efficient model adaptation by removing unnecessary or outdated knowledge.\n",
        "        Adaptation to New Information: When new data becomes available, unlearning can allow the model to forget old or irrelevant data to maintain relevance without the need for full retraining.\n",
        "\n",
        "    Improving Fairness:\n",
        "        Bias Reduction: Machine unlearning can be used to remove biased or harmful training data, making models more fair and unbiased. If a model was trained on data that led to biased predictions, unlearning can help eliminate that effect.\n",
        "\n",
        "    Security:\n",
        "        Preventing Leakage of Private Data: In some cases, models may unintentionally memorize sensitive information (e.g., overfitting to user data), leading to privacy risks. Machine unlearning helps ensure that such information is forgotten.\n",
        "\n",
        "Approaches to Machine Unlearning\n",
        "\n",
        "There are several ways to achieve machine unlearning:\n",
        "\n",
        "    Gradient-Based Unlearning:\n",
        "        After a model has been trained, we can apply gradient-based methods to undo the learning that occurred from specific data points. This requires calculating gradients and applying them in a way that \"erases\" the influence of the forgotten data.\n",
        "\n",
        "    Data Removal and Fine-Tuning:\n",
        "        Involves removing the specific data points from the dataset and then fine-tuning the model to adjust to the new data without that specific information. It can be less computationally expensive than retraining the model from scratch but might still not be as precise.\n",
        "\n",
        "    Model Distillation:\n",
        "        A new, smaller model is trained to mimic the original model's behavior, while selectively ignoring or removing the influence of the data that should be forgotten.\n",
        "\n",
        "    Reversible Learning Algorithms:\n",
        "        Some algorithms are designed to be reversible or have mechanisms to unlearn certain aspects of the training data. These algorithms are built with the idea of forgetting as a fundamental part of their design.\n",
        "\n",
        "Use Cases for Machine Unlearning\n",
        "\n",
        "    Personalized Systems:\n",
        "        In recommendation systems or user-facing applications, users may want the system to forget certain behaviors or preferences. Unlearning can allow the system to forget user preferences or recommendations tied to certain users.\n",
        "\n",
        "    Legal and Ethical Considerations:\n",
        "        When machine learning models are used to make decisions that affect people's lives (e.g., credit scoring, hiring, healthcare), unlearning can help ensure that outdated or irrelevant data doesn't influence critical decisions.\n",
        "\n",
        "    Fairness Audits:\n",
        "        If a model is found to be unfair or discriminatory due to certain training data (e.g., gender or racial bias), machine unlearning can be applied to remove those biased data points from the model's learning process.\n",
        "\n",
        "Example Use Case: Forgetting a Specific Class\n",
        "\n",
        "In your previous example with the neural network, we performed machine unlearning by focusing on \"forgetting\" a specific class (vehicle). By applying techniques like gradient clipping or selective training, the model was encouraged to forget the information related to this class while maintaining knowledge of other classes (e.g., animal, nature). The main use here could be:\n",
        "\n",
        "    Removing Specific Knowledge: For example, a recommendation system that \"forgets\" a particular user's preferences while still providing accurate recommendations to other users.\n",
        "    Adapting to Changing Data: If the vehicle class becomes irrelevant over time (e.g., an update in the dataset where vehicles are no longer of interest), unlearning can allow the model to adapt to this change.\n",
        "\n",
        "Summary of Key Benefits:\n",
        "\n",
        "    Privacy and Security: Forget sensitive or personal data without retraining the model.\n",
        "    Efficiency: Avoid full retraining and instead focus on selective forgetting.\n",
        "    Fairness: Mitigate biases by removing harmful data points.\n",
        "    Compliance: Ensure that models can adhere to regulations such as GDPR.\n",
        "\n",
        "Machine unlearning is an emerging area, and there are still many challenges and open problems in making it more efficient and widely applicable across different types of models and use cases."
      ],
      "metadata": {
        "id": "biEUBxOghPxE"
      }
    }
  ]
}